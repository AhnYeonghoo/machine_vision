# -*- coding: utf-8 -*-
"""12_CNN_mnist

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qbwdS2dZPzkmp_F_EN2VKswma_GY6i_X
"""

import torch
import torch.nn as nn

conv1 = nn.Conv2d(1, 1, 3, padding=1)
input1 = torch.Tensor(1, 1, 5, 5)
out1 = conv1(input1)
out1.shape

conv1 = nn.Conv2d(1, 1, 3, padding=1)
input1 = torch.Tensor(1,1,5,5)
pool1 = nn.MaxPool2d(2)
out1 = conv1(input1)
print(out1.shape)
out2 = pool1(out1)
print(out2.shape)

conv1 = nn.Sequential(
    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
    nn.LeakyReLU(0.1),
    nn.BatchNorm2d(32),
    nn.MaxPool2d(2)
)
input1 = torch.Tensor(1, 1, 28, 28)
out1 = conv1(input1)
print(out1.shape)

conv1 = nn.Sequential(
    nn.Conv2d(1,32,kernel_size=3,stride=1,padding=1),
    nn.LeakyReLU(0.1),
    nn.BatchNorm2d(32),
    nn.MaxPool2d(2),
    
    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
    nn.LeakyReLU(0.1),
    nn.BatchNorm2d(64),
    nn.MaxPool2d(2),
    
    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
    nn.LeakyReLU(0.1),
    nn.BatchNorm2d(128),
    nn.MaxPool2d(2)
)

input1 = torch.Tensor(1, 1, 28, 28)
out1 = conv1(input1)
out2 = out1.view(out1.size(0), -1) # Platten
print(out1.shape, out2.shape, 128 * 3 * 3)

class CNNModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv_layers = nn.Sequential(
        nn.Conv2d(1,32,kernel_size=3,stride=1,padding=1),
        nn.LeakyReLU(0.1),
        nn.BatchNorm2d(32),
        nn.MaxPool2d(2),

        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(0.1),
        nn.BatchNorm2d(64),
        nn.MaxPool2d(2),

        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(0.1),
        nn.BatchNorm2d(128),
        nn.MaxPool2d(2)
    )
    
    self.linear_layers = nn.Sequential(
        nn.Linear(3 * 3 * 128, 128),
        nn.LeakyReLU(0.1),
        nn.BatchNorm1d(128),
        nn.Linear(128, 64),
        nn.LeakyReLU(0.1),
        nn.BatchNorm1d(64),
        nn.Linear(64, 10),
        nn.LogSoftmax(dim=-1)
    )

  
  def forward(self,x):
    x = self.conv_layers(x)
    x = x.view(x.size(0), -1)
    x = self.linear_layers(x)
    return x

from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split
import numpy as np
from copy import deepcopy

device = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(1)
if device == 'cuda':
  torch.cuda.manual_seed_all(1)
print(device)

train_rawdata = datasets.MNIST(
    root = 'dataset',
    train=True,
    download=True,
    transform=transforms.ToTensor())

test_dataset = datasets.MNIST(
    root = 'dataset',
    train=False,
    download=True,
    transform=transforms.ToTensor())

print(len(train_rawdata))
print(len(test_dataset))

validation_rate = 0.2
train_indices, val_indices, _, _ = train_test_split(
    range(len(train_rawdata)),
    train_rawdata.targets,
    stratify=train_rawdata.targets,
    test_size=validation_rate
)

train_dataset = Subset(train_rawdata, train_indices)
validation_dataset = Subset(train_rawdata, val_indices)

minibatch_size = 128
train_batches = DataLoader(train_dataset, batch_size=minibatch_size, shuffle=True)
val_batches = DataLoader(validation_dataset, batch_size=minibatch_size, shuffle=True)
test_batches = DataLoader(test_dataset, batch_size=minibatch_size, shuffle=False)

model = CNNModel().to(device)
loss_func = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters())

def train_model(model, early_stop, epochs, progress_interval):
  
  train_losses, valid_losses, lowest_loss = list(), list(), np.inf

  for epoch in range(epochs):
    model.train()
    for x_minibatch, y_minibatch in train_batches:
      x_minibatch = x_minibatch.to(device)
      y_minibatch = y_minibatch.to(device)
      y_minibatch_pred = model(x_minibatch)
      loss = loss_func(y_minibatch_pred, y_minibatch)
      
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      train_losses.append(loss.item())

  
    model.eval()
    with torch.no_grad():
      for x_minibatch, y_minibatch in val_batches:
        x_minibatch = x_minibatch.to(device)
        y_minibatch = y_minibatch.to(device)
        y_minibatch_pred = model(x_minibatch)
        loss = loss_func(y_minibatch_pred, y_minibatch)
        valid_losses.append(loss.item())
  
    if valid_losses[-1] < lowest_loss:
      lowest_loss = valid_losses[-1]
      lowest_epoch = epoch
      best_model = deepcopy(model.state_dict())
    else:
      if early_stop > 0 and lowest_epoch + early_stop < epoch:
        print("Early stopped", epoch, 'epochs')
        model.load_state_dict(best_model)
        break
    if (epoch % progress_interval) == 0:
      print(train_losses[-1], valid_losses[-1], lowest_loss, lowest_epoch, epoch)
  
  model.load_state_dict(best_model)
  return model, lowest_loss, train_losses, valid_losses

epochs = 100
progress_interval = 3
early_stop = 30

model, lowest_loss, train_losses, valid_losses = train_model(model, early_stop, epochs, progress_interval)

