# -*- coding: utf-8 -*-
"""05_gradient_descent_example

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jZgJHseNPidFNX51LouuM4iklP3t9xFp
"""

import torch 
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.datasets import load_diabetes

diabetes_data = load_diabetes()
print(diabetes_data.DESCR)

x = torch.from_numpy(np.array(diabetes_data.data[:, :-1], dtype=np.float32)) # 맨 끝 인덱스 전까지만 : 맨 끝 인덱스는 target value
y = torch.from_numpy(np.array(diabetes_data.data[:, [-1]], dtype=np.float32)) # x와 반대로 target value인 맨 끝 인덱스만 가져옴

print(f'x.shape: {x.shape}')
print(f'y.shape: {y.shape}')

class LinearRegressionModel(nn.Module):
  def __init__(self, input_dim, output_dim):
    super().__init__()
    self.linear = nn.Linear(input_dim, output_dim)
  
  def forward(self, x):
    return self.linear(x)

model = LinearRegressionModel(x.size(1), y.size(1)) # x 사이즈의 인덱스 1인 9, y 사이즈의 인덱스 1인 1을 가져옴

learning_rate = 0.01
nb_epochs = 10000
opimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(nb_epochs+1):
  
  y_pred = model(x)
  loss = F.mse_loss(y_pred, y)
  
  opimizer.zero_grad()
  loss.backward()
  opimizer.step()

y_pred.shape

print(loss)
for param in model.parameters():
  print(param)

df = pd.DataFrame(torch.cat([y, y_pred], dim=1).detach().numpy(), columns=["y", "y_pred"])

import plotly.graph_objects as go
import plotly.offline as pyo
pyo.init_notebook_mode()

fig = go.Figure()

fig.add_trace(go.Scatter(x=df.index, y=df['y'], mode='markers', name='y'))
fig.add_trace(go.Scatter(x=df.index, y=df['y_pred'], mode='markers', name='y_pred'))

fig.update_layout(yaxis_range=[-0.5,0.5])
fig.show()

