# -*- coding: utf-8 -*-
"""06_1_DNN_Pytorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kkLB7pZ2mzWUEMjCWQdyEUeEQNVk2xm6
"""

import torch
import torch.nn as nn

class LinearRegressionModel(nn.Module):
  def __init__(self, input_dim, output_dim):
    super().__init__()
    self.linear = nn.Linear(in_features=input_dim, out_features=output_dim)
    self.activation = nn.Sigmoid()
    
  def forward(self, x):
    return self.activation(self.linear(x))

x = torch.ones(4)
y = torch.zeros(3)
model =LinearRegressionModel(4, 3)
loss_function = nn.MSELoss()

learning_rate = 0.01
nb_epochs = 1000
opimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(nb_epochs+1):

  y_pred = model(x)
  loss = loss_function(y_pred, y)
  
  opimizer.zero_grad()
  loss.backward()
  opimizer.step()

print(loss)

for param in model.parameters():
  print(param)

class LinearRegressionModel(nn.Module):
  
  def __init__(self, input_dim, output_dim):
    super().__init__()
    self.linear1 = nn.Linear(input_dim, 10)
    self.linear2 = nn.Linear(10, 10)
    self.linear3 = nn.Linear(10, 10)
    self.linear4 = nn.Linear(10, output_dim)
    self.activation = nn.LeakyReLU(0.1)
  
  def forward(self, x):
    hidden = self.activation(self.linear1(x))
    hidden = self.activation(self.linear2(hidden))
    hidden = self.activation(self.linear3(hidden))
    y = self.linear4(hidden)
    return y

x = torch.ones(4)
y = torch.zeros(3)
model = LinearRegressionModel(4, 3)
loss_function = nn.MSELoss()
learning_rate = 0.01
nb_epochs = 1000
opimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(nb_epochs+1):
  y_pred = model(x)
  loss = loss_function(y_pred, y)
  
  opimizer.zero_grad()
  loss.backward()
  opimizer.step()

print(loss)

for param in model.parameters():
  print(param)

input_dim = x.size(0)
output_dim = y.size(0)
x = torch.ones(4)
y = torch.zeros(3)
model = nn.Sequential(
    nn.Linear(input_dim, 10),
    nn.LeakyReLU(0.1),
    nn.Linear(10, 10),
    nn.LeakyReLU(0.1),
    nn.Linear(10, 10),
    nn.LeakyReLU(0.1),
    nn.Linear(10, output_dim)
)

loss_function = nn.MSELoss()
learning_rate = 0.01
epochs = 1000
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(epochs+1):
  y_pred = model(x)
  loss = loss_function(y_pred, y)
  
  opimizer.zero_grad()
  loss.backward()
  opimizer.step()

print(loss)

for param in model.parameters():
  print(param)

data1 = torch.randn(3, 4)
print(data1)
indices = torch.tensor([1,2])
print(indices)
print(torch.index_select(data1, 0, indices))
print(torch.index_select(data1, 1, indices))

x = torch.ones(5000, 10)
y = torch.zeros(5000, 1)
learning_rate = 0.01
epochs = 1000
minibatch_size = 256
input_dim = x.size(-1)
output_dim = y.size(-1)

model = nn.Sequential(
    nn.Linear(input_dim, 10),
    nn.LeakyReLU(0.1),
    nn.Linear(10, 8),
    nn.LeakyReLU(0.1),
    nn.Linear(8, 6),
    nn.LeakyReLU(0.1),
    nn.Linear(6, output_dim)
)

loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

indices = torch.randperm(x.size(0))
print(indices)
x_batch_list = torch.index_select(x, 0, index=indices)
y_batch_list = torch.index_select(y, 0, index=indices)
x_batch_list = x_batch_list.split(minibatch_size, 0)
y_batch_list = y_batch_list.split(minibatch_size, 0)
print(len(x), len(y))

print(len(x_batch_list), len(y_batch_list))

for index in range(epochs):
  indices = torch.randperm(x.size(0))

  x_batch_list = torch.index_select(x, 0, index=indices)
  y_batch_list = torch.index_select(y, 0, index=indices)
  x_batch_list = x_batch_list.split(minibatch_size, 0)
  y_batch_list = y_batch_list.split(minibatch_size, 0)
  
  for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):
    y_minibatch_pred = model(x_minibatch)
    loss = loss_function(y_minibatch_pred, y_minibatch)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(loss)
for param in model.parameters():
  print(param)

