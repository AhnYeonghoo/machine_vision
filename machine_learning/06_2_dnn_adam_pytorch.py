# -*- coding: utf-8 -*-
"""06_2_DNN_Adam_Pytorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E7gZGdL4UkX3_OKIilWNDhuN-f8STMEO
"""

import torch
import torch.nn as nn

x = torch.ones(5000, 10)
y = torch.zeros(5000, 1)
learning_rate = 0.01
epochs = 1000
minibatch_size = 256
input_dim = x.size(-1)
output_dim = y.size(-1)

model = nn.Sequential(
    nn.Linear(input_dim, 10),
    nn.LeakyReLU(0.1),
    nn.Linear(10, 8),
    nn.LeakyReLU(0.1),
    nn.Linear(8, 6),
    nn.LeakyReLU(0.1),
    nn.Linear(6,output_dim)
)

loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters())

indices = torch.randperm(x.size(0))
print(indices)

x_batch_list = torch.index_select(x, 0, index=indices)
y_batch_list = torch.index_select(y, 0, index=indices)
x_batch_list = x_batch_list.split(minibatch_size, dim=0)
y_batch_list = y_batch_list.split(minibatch_size, dim=0)
print(len(x_batch_list), len(y_batch_list))

for index in range(epochs):
  indices = torch.randperm(x.size(0))
  
  x_batch_list = torch.index_select(x, 0, index=indices)
  y_batch_list = torch.index_select(y, 0, index=indices)
  x_batch_list = x_batch_list.split(minibatch_size, dim=0)
  y_batch_list = y_batch_list.split(minibatch_size, dim=0)

  for x_batch, y_batch in zip(x_batch_list, y_batch_list):
    y_batch_pred = model(x_batch)
    loss = loss_function(y_batch_pred, y_batch)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  
print(loss)
for param in model.parameters():
  print(param)

